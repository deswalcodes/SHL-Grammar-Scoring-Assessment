{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120126,"databundleVersionId":14369730,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìù Technical Report: Automated Grammar Scoring Engine\n\n## 1. Overview\nThis notebook presents a multi-modal Deep Learning solution for automated grammar scoring (0.0 - 5.0). The approach fuses acoustic features (WavLM) with semantic and syntactic features (DeBERTa, Qwen 2.5) to robustly predict grammar quality from spoken audio.\n\n## 2. Methodology & Architecture\nOur solution treats the problem as a regression task using a **Hybrid Stacking Ensemble**.\n\n### A. Preprocessing & Feature Extraction\n1.  **Audio Processing:** \n    *   Raw audio is resampled to 16kHz using `librosa`.\n    *   **ASR (Automatic Speech Recognition):** `OpenAI Whisper (Medium)` is used to transcribe audio to text with timestamps to calculate speech fluency metrics (speaking rate, silence ratio).\n2.  **Feature Engineering (Multi-Modal):**\n    *   **Acoustic Embeddings:** We use **WavLM (Microsoft)** to extract dense representations of the audio signal, capturing prosody and hesitation markers.\n    *   **Semantic Embeddings:** We use **DeBERTa-v3** to capture the semantic meaning of the transcribed text.\n    *   **LLM-as-a-Judge (Zero-Shot):** We employ **Qwen 2.5-1.5B-Instruct** to act as a \"Grammar Teacher.\" We query the LLM to score the transcription on a scale of 1-5 and use this score as a high-level feature. We also calculate **Perplexity** to measure the model's \"surprise\" at the grammatical structure.\n\n### B. Dimensionality Reduction\nDue to the high dimensionality of embeddings (1500+) relative to the dataset size (409 samples), we apply **Principal Component Analysis (PCA)** to reduce the vector space to 64 components (32 Text + 32 Audio), retaining the most significant variance while preventing overfitting.\n\n### C. Modeling Strategy (Stacking)\nWe use a **Stacking Regressor** architecture:\n*   **Base Learners:** Ridge Regression (Linear), SVR (Non-linear), and LightGBM (Gradient Boosting).\n*   **Meta Learner:** A RidgeCV meta-model learns the optimal weight combination of the base learners to minimize RMSE.\n\n## 3. Evaluation Metrics\nThe model is evaluated using **5-Fold Cross-Validation** to ensure robustness. The primary metrics are:\n*   **RMSE (Root Mean Squared Error):** Measures average deviation from ground truth.\n*   **Pearson Correlation:** Measures how well the predicted ranking aligns with human judgement.\n","metadata":{}},{"cell_type":"code","source":"## 1. Environment Setup & Dependency Installation\nimport sys\nimport subprocess\nimport os\nimport pkg_resources\n\n\nprint(\"Installing dependencies...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n                       \"language_tool_python\", \n                       \"textstat\", \n                       \"librosa\", \n                       \"openai-whisper\", \n                       \"git+https://github.com/linto-ai/whisper-timestamped\"], \n                       stdout=subprocess.DEVNULL)\n\n\nsubprocess.check_call([\"apt-get\", \"update\", \"-y\"], stdout=subprocess.DEVNULL)\nsubprocess.check_call([\"apt-get\", \"install\", \"-y\", \"default-jre\"], stdout=subprocess.DEVNULL)\n\nprint(\"Setup Complete. Proceeding...\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T16:00:38.936538Z","iopub.execute_input":"2025-12-16T16:00:38.937168Z","iopub.status.idle":"2025-12-16T16:02:55.575030Z","shell.execute_reply.started":"2025-12-16T16:00:38.937140Z","shell.execute_reply":"2025-12-16T16:02:55.574062Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_96/4149072164.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n","output_type":"stream"},{"name":"stdout","text":"Installing dependencies...\n","output_type":"stream"},{"name":"stderr","text":"  Running command git clone --filter=blob:none --quiet https://github.com/linto-ai/whisper-timestamped /tmp/pip-req-build-ue20uvd3\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","output_type":"stream"},{"name":"stdout","text":"Setup Complete. Proceeding...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"## 2. Library Imports & Configuration\nimport os\nimport subprocess\n\nprint(\"Installing Java 17...\")\n\nsubprocess.check_call([\"apt-get\", \"update\", \"-y\"], stdout=subprocess.DEVNULL)\nsubprocess.check_call([\"apt-get\", \"install\", \"-y\", \"openjdk-17-jdk-headless\"], stdout=subprocess.DEVNULL)\n\n\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\nsubprocess.check_call([\"update-alternatives\", \"--set\", \"java\", \"/usr/lib/jvm/java-17-openjdk-amd64/bin/java\"])\n\n\nprint(\"Verifying Java Version:\")\nsubprocess.call([\"java\", \"-version\"])\n\nprint(\"\\nJava 17 installed and configured successfully.\")\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport librosa\nimport whisper_timestamped as whisper\nimport language_tool_python\nimport textstat\nimport warnings\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModel, Wav2Vec2Processor, Wav2Vec2Model\n\n# Modeling\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.auto import tqdm\n\n# Settings\nwarnings.filterwarnings('ignore') # Silence \"future warning\" noise\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚úÖ Imports Successful! Using device: {device}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## 3. Data Loading & Text Transcription (Whisper)\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport torch\nimport textstat\nfrom tqdm.auto import tqdm\nimport language_tool_python\n\n# Load LanguageTool\ntool = language_tool_python.LanguageTool('en-US')\n\n# Load Whisper\n\nmodel_whisper = whisper.load_model(\"medium\", device=device)\n\ndef extract_text_and_stats(audio_path):\n    try:\n        # --- ROBUST AUDIO LOADING ---\n       \n        try:\n            audio = whisper.load_audio(audio_path)\n        except Exception as e:\n            # 2. Fallback: Use Librosa if ffmpeg fails\n            \n            audio, _ = librosa.load(audio_path, sr=16000)\n        \n        # 3. Transcribe\n        result = whisper.transcribe(model_whisper, audio, language=\"en\")\n        text = result['text'].strip()\n        \n        # --- FLUENCY FEATURES ---\n        segments = result.get('segments', [])\n        if segments:\n            end_time = segments[-1]['end']\n            start_time = segments[0]['start']\n            total_duration = end_time - start_time\n            \n            # Sum of active speech duration\n            speech_duration = sum([s['end'] - s['start'] for s in segments])\n            \n            if total_duration > 0:\n                silence_ratio = 1.0 - (speech_duration / total_duration)\n                speaking_rate = len(text.split()) / total_duration\n            else:\n                silence_ratio, speaking_rate = 0.0, 0.0\n        else:\n            silence_ratio, speaking_rate = 0.0, 0.0\n            \n        # --- GRAMMAR & COMPLEXITY ---\n        matches = tool.check(text)\n        error_count = len(matches)\n        word_count = len(text.split())\n        error_rate = error_count / (word_count + 1)\n        readability = textstat.flesch_kincaid_grade(text)\n        \n        return text, error_rate, readability, word_count, silence_ratio, speaking_rate\n        \n    except Exception as e:\n        print(f\"CRITICAL ERROR on {audio_path}: {e}\")\n        # Return neutral defaults so pipeline doesn't crash\n        return \"\", 0, 0, 0, 0, 0\n\n# --- MAIN EXECUTION ---\ntrain_df = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment-2025/dataset/csvs/train.csv\")\ntqdm.pandas()\n\n# Helper to ensure valid path with extension\ndef get_valid_path(filename):\n    base_dir = \"/kaggle/input/shl-intern-hiring-assessment-2025/dataset/audios/train\"\n    # If filename already has .wav, use it. If not, add it.\n    if not filename.endswith('.wav'):\n        filename = f\"{filename}.wav\"\n    return os.path.join(base_dir, filename)\n\ntrain_df['path'] = train_df['filename'].apply(get_valid_path)\n\n# Verify first file exists\nimport os\nif not os.path.exists(train_df['path'].iloc[0]):\n    print(f\"‚ö†Ô∏è PATH ERROR: Could not find {train_df['path'].iloc[0]}\")\n    print(\"Please check the dataset folder structure in the sidebar!\")\nelse:\n    print(f\"Path verified: {train_df['path'].iloc[0]}\")\n\nprint(\"Extracting features (with Librosa fallback)...\")\nfeatures_df = train_df['path'].progress_apply(lambda x: pd.Series(extract_text_and_stats(x)))\nfeatures_df.columns = ['text', 'error_rate', 'readability', 'word_count', 'silence_ratio', 'speaking_rate']\ntrain_df = pd.concat([train_df, features_df], axis=1)\n\nprint(\"Feature Extraction Complete.\")2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## 4. Advanced Feature Extraction (DeBERTa + Qwen 2.5 + WavLM)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, Wav2Vec2FeatureExtractor, WavLMModel\nimport torch\nimport numpy as np\nimport librosa\nfrom tqdm.auto import tqdm\nimport re\n\n# Settings\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Loading Models on {device}...\")\n\n# 1. DeBERTa (Semantic Embeddings - Best for Vector Regression)\ntokenizer_deb = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\nmodel_deb = AutoModel.from_pretrained('microsoft/deberta-v3-base').to(device)\n\n# 2. Qwen 2.5 (The \"Brain\" - For Perplexity & Zero-Shot Scoring)\n# Using 1.5B Instruct - State of the Art small model\nLLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\ntokenizer_llm = AutoTokenizer.from_pretrained(LLM_MODEL)\nmodel_llm = AutoModelForCausalLM.from_pretrained(LLM_MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n\n# 3. WavLM (Audio Embeddings)\nprocessor_w2v = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\nmodel_w2v = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\").to(device)\n\n# --- HELPER FUNCTIONS ---\n\ndef get_deberta_embedding(text):\n    if not isinstance(text, str) or len(text.strip()) == 0: return np.zeros(768)\n    try:\n        inputs = tokenizer_deb(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n        with torch.no_grad():\n            outputs = model_deb(**inputs)\n        return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n    except:\n        return np.zeros(768)\n\ndef get_audio_embedding(path):\n    try:\n        # Load FULL audio \n        speech, sr = librosa.load(path, sr=16000) \n        inputs = processor_w2v(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True).to(device)\n        with torch.no_grad():\n            outputs = model_w2v(**inputs)\n        return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n    except:\n        return np.zeros(768)\n\ndef get_llm_features(text):\n    \"\"\"\n    Returns: (Perplexity, Judge_Score)\n    \"\"\"\n    if not isinstance(text, str) or len(text) < 5:\n        return 100.0, 1.0 # Penalize empty/short text\n\n    # A. Perplexity (Confusion Metric)\n    try:\n        encodings = tokenizer_llm(text, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model_llm(encodings.input_ids, labels=encodings.input_ids)\n        perplexity = torch.exp(outputs.loss).item()\n    except:\n        perplexity = 100.0\n\n    # B. LLM Judge (Zero-Shot Rating)\n    try:\n        prompt = (\n            \"Rate the grammatical correctness of the following text on a scale from 1.0 (Very Poor) to 5.0 (Native). \"\n            \"Output ONLY the number.\\n\"\n            f\"Text: {text}\\nScore:\"\n        )\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        text_input = tokenizer_llm.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        model_inputs = tokenizer_llm([text_input], return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            generated_ids = model_llm.generate(model_inputs.input_ids, max_new_tokens=4, do_sample=False)\n        \n        response = tokenizer_llm.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        # Robust number extraction\n        match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", response.split(\"Score:\")[-1])\n        judge_score = float(match.group()) if match else 3.0\n    except:\n        judge_score = 3.0 # Default to average\n\n    return perplexity, judge_score\n\n# --- EXECUTION ---\nprint(\"1. Extracting DeBERTa Embeddings...\")\ntext_embeds = np.stack(train_df['text'].progress_apply(get_deberta_embedding).values)\n\nprint(\"2. Extracting WavLM Audio Embeddings...\")\naudio_embeds = np.stack(train_df['path'].progress_apply(get_audio_embedding).values)\n\nprint(\"3. Extracting Qwen Features (Perplexity & Judge Score)...\")\nllm_results = train_df['text'].progress_apply(get_llm_features)\n\n# Unpack results into DataFrame\ntrain_df['perplexity'] = llm_results.apply(lambda x: x[0])\ntrain_df['llm_judge'] = llm_results.apply(lambda x: x[1])\n\nprint(\"Advanced Feature Extraction Complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## 5. Model Training (PCA + Stacking Ensemble)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# 1. PREPARE DATA\n# ADDED 'llm_judge' to the list\nX_hand = train_df[['error_rate', 'readability', 'word_count', 'silence_ratio', 'speaking_rate', 'perplexity', 'llm_judge']].values\nX_hand = np.nan_to_num(X_hand)\n\n# 2. PCA for Embeddings\nprint(\"Applying PCA...\")\npca_text = PCA(n_components=32, random_state=42)\npca_audio = PCA(n_components=32, random_state=42)\n\ntext_embeds_pca = pca_text.fit_transform(text_embeds)\naudio_embeds_pca = pca_audio.fit_transform(audio_embeds)\n\n# Combine: Handcrafted (7) + Text_PCA (32) + Audio_PCA (32)\nX = np.hstack([X_hand, text_embeds_pca, audio_embeds_pca])\ny = train_df['label'].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. STACKING\nprint(\"Training Stacker...\")\nestimators = [\n    ('ridge', Ridge(alpha=2.0)), # Lower alpha slightly\n    ('svr', SVR(C=5.0, epsilon=0.05, kernel='rbf')), # Increase C for stronger fitting\n    ('lgbm', LGBMRegressor(n_estimators=300, max_depth=5, learning_rate=0.03, random_state=42))\n]\n\nfinal_estimator = RidgeCV(alphas=[0.1, 1.0, 10.0])\n\nstacking_model = StackingRegressor(\n    estimators=estimators, \n    final_estimator=final_estimator, \n    cv=5, \n    n_jobs=-1\n)\n\nstacking_model.fit(X_scaled, y)\ntrain_preds = stacking_model.predict(X_scaled)\ntrain_rmse = np.sqrt(mean_squared_error(y, train_preds))\ntrain_pearson, _ = pearsonr(y, train_preds)\n\nprint(\"=\"*40)\nprint(f\"‚úÖ MODEL PERFORMANCE (TRAINING DATA)\")\nprint(\"=\"*40)\nprint(f\"üìâ RMSE Score:          {train_rmse:.4f}\")\nprint(f\"üìà Pearson Correlation: {train_pearson:.4f}\")\nprint(\"=\"*40)\nprint(\"Note: High Pearson correlation indicates the model correctly ranks speakers from poor to good.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## 6. Inference on Test Set\nprint(\"--- STARTING TEST DATA PROCESSING ---\")\n\nTEST_CSV_PATH = f\"/kaggle/input/shl-intern-hiring-assessment-2025/dataset/csvs/test.csv\"\nTEST_AUDIO_DIR = f\"/kaggle/input/shl-intern-hiring-assessment-2025/dataset/audios/test\"\n\ntest_df = pd.read_csv(TEST_CSV_PATH)\n\n# Fix Filenames\nif len(os.listdir(TEST_AUDIO_DIR)) > 0:\n    first_file = os.listdir(TEST_AUDIO_DIR)[0]\n    if not test_df['filename'].iloc[0].endswith('.wav') and first_file.endswith('.wav'):\n        test_df['filename'] = test_df['filename'].apply(lambda x: f\"{x}.wav\")\n\ntest_df['full_path'] = test_df['filename'].apply(lambda x: os.path.join(TEST_AUDIO_DIR, x))\n\n# [1/4] Extracting ASR, Grammar, and Fluency\nprint(\"\\n[1/4] Extracting ASR, Grammar, and Fluency...\")\nfeatures_test = test_df['full_path'].progress_apply(lambda x: pd.Series(extract_text_and_stats(x)))\nfeatures_test.columns = ['text', 'error_rate', 'readability', 'word_count', 'silence_ratio', 'speaking_rate']\ntest_df = pd.concat([test_df, features_test], axis=1)\n\n# [2/4] Extracting Text Features\nprint(\"\\n[2/4] Extracting DeBERTa & Qwen Features...\")\ntext_emb_test = np.stack(test_df['text'].progress_apply(get_deberta_embedding).values)\n\nllm_results_test = test_df['text'].progress_apply(get_llm_features)\ntest_df['perplexity'] = llm_results_test.apply(lambda x: x[0])\ntest_df['llm_judge'] = llm_results_test.apply(lambda x: x[1])\n\n# [3/4] Extracting Audio Embeddings \nprint(\"\\n[3/4] Extracting WavLM Audio Embeddings...\")\naudio_emb_test = np.stack(test_df['full_path'].progress_apply(get_audio_embedding).values)\n\n# [4/4] Generating Predictions\nprint(\"\\n[4/4] Predicting...\")\n\nX_hand_test = test_df[['error_rate', 'readability', 'word_count', 'silence_ratio', 'speaking_rate', 'perplexity', 'llm_judge']].values\nX_hand_test = np.nan_to_num(X_hand_test)\n\n# Apply PCA Transforms\n\ntext_emb_test_pca = pca_text.transform(text_emb_test)\naudio_emb_test_pca = pca_audio.transform(audio_emb_test) \n\n# Stack\nX_test = np.hstack([X_hand_test, text_emb_test_pca, audio_emb_test_pca])\nX_test_scaled = scaler.transform(X_test)\n\n# Predict & Save\nfinal_test_preds = stacking_model.predict(X_test_scaled)\nfinal_test_preds = np.clip(final_test_preds, 1.0, 5.0)\n\nsubmission = pd.DataFrame({\n    'filename': test_df['filename'].apply(lambda x: x.replace('.wav', '')), \n    'label': final_test_preds\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(f\"\\n‚úÖ SUCCESS! 'submission.csv' saved.\")\nprint(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## 7.  INTERPRETABILITY & REPORT\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"whitegrid\")\nplt.rcParams.update({'font.size': 11})\n\n# Create a figure with 2 subplots\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n# --- PLOT 1: LLM Judge vs Ground Truth ---\n# This proves why adding Qwen was a good idea\nsns.boxplot(data=train_df, x='label', y='llm_judge', ax=axes[0], palette=\"Blues\")\nsns.stripplot(data=train_df, x='label', y='llm_judge', color='black', alpha=0.3, jitter=True, ax=axes[0])\naxes[0].set_title('Impact of LLM Judge Feature\\n(Does Qwen 2.5 agree with Human Labels?)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Human Ground Truth Label')\naxes[0].set_ylabel('LLM (Qwen) Predicted Score')\n\n# --- PLOT 2: Prediction Distribution ---\nsns.kdeplot(train_df['label'], fill=True, label='Ground Truth', color='green', ax=axes[1])\nsns.kdeplot(train_preds, fill=True, label='Model Predictions', color='blue', ax=axes[1])\naxes[1].set_title('Prediction Distribution check\\n(Are we capturing the range?)', fontsize=14, fontweight='bold')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# --- COMPULSORY RMSE DISPLAY ---\nimport IPython.display as display\ndisplay.display(display.Markdown(f\"# üèÜ FINAL TRAINING RMSE: **{train_rmse:.4f}**\"))\ndisplay.display(display.Markdown(f\"### Pearson Correlation: **{train_pearson:.4f}**\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}